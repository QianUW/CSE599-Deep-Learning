{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\", \"r\")\nlines = f.readlines()\ni = 0;\nfor line in lines:\n    i = i + 1\n    if(i > 10):\n        break\n    print(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nfile = open(\"/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\", 'rb')\n\ndata = pickle.load(file)\n\nfile.close()\ni = 0;\nprint(type(data))\nfor key in data:\n    i = i + 1\n    if(i > 10):\n        break\n    print(key, type(data[key]), data[key])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.add(1,2))\nprint(tf.square(5).dtype)\nprint(tf.reduce_sum([1, 2, 3]))\nprint(tf.square(2) + tf.square(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tf.matmul([[1]], [[2, 3]])\nprint(x)\nprint(x.shape)\nprint(type(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nndarray = np.ones([3,3])\ntensor = tf.multiply(ndarray, 42)\nprint(tensor)\nprint(np.add(tensor, 1))\nprint(tensor.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tf.random.uniform([3, 3])\nprint(tf.config.experimental.list_physical_devices(\"GPU\"))\nprint(x.device.endswith('GPU:0'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport tensorflow as tf\n\ndef time_matmul(x):\n    start = time.time()\n    for loop in range(10):\n        tf.matmul(x, x)\n    result = time.time() - start\n    print(\"10 loops: {:0.2f}ms\".format(1000*result))\n\nprint(\"On CPU\")\nwith tf.device(\"CPU:0\"):\n    x = tf.random.uniform([1000, 1000])\n    assert x.device.endswith(\"CPU:0\")\n    time_matmul(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_tensors = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6])\nimport tempfile\n_, filename = tempfile.mkstemp()\n\nwith open(filename, 'w') as f:\n    f.write(\"\"\"Line 1\n    Line2\n    Line3\n    \"\"\")\n\nds_file = tf.data.TextLineDataset(filename)\n\nds_tensors = ds_tensors.shuffle(3).batch(3)\n\nds_file = ds_file.batch(2)\n\nprint('Elements of ds_tensor:')\nfor x in ds_tensors:\n    print(x)\n\nprint('\\nElements in ds_file')\nfor x in ds_file:\n    print(x)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tf.ones((2,2))\n\nwith tf.GradientTape() as t:\n    t.watch(x)\n    y = tf.reduce_sum(x)\n    z = tf.multiply(y, y)\n\ndz_dx = t.gradient(z, x)\nfor i in [0, 1]:\n    for j in [0, 1]:\n        assert dz_dx[i][j].numpy() == 8.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tf.zeros([10,10])\nx += 2\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v = tf.Variable(1.0)\nprint(v)\nv.assign(3.0)\nprint(v)\nv.assign(tf.square(v))\nassert v.numpy() == 9.0\nprint(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\nclass Model(object):\n    def __init__(self):\n        self.W = tf.Variable(5.0)\n        self.b = tf.Variable(0.0)\n    def __call__(self, x):\n        return self.W * x + self.b\n\nmodel = Model()\n\nassert model(3.0).numpy() == 15.0\n\ndef loss(predict_y, target_y):\n    return tf.reduce_mean(tf.square(predict_y - target_y))\n\nTRUE_W = 3.0\nTRUE_B = 2.0\nNUM_EXAMPLES = 1000\n\ninputs = tf.random.normal(shape = [NUM_EXAMPLES])\nnoise   = tf.random.normal(shape=[NUM_EXAMPLES])\noutputs = inputs * TRUE_W + TRUE_B + noise\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(inputs, outputs, c='b')\nplt.scatter(inputs, model(inputs), c='r')\nplt.show()\n\nprint('Current loss: %1.6f' % loss(model(inputs), outputs).numpy())\n\n\n\ndef train(model, inputs, outputs, learning_rate):\n    with tf.GradientTape() as t:\n        current_loss = loss(model(inputs), outputs)\n    grads = t.gradient(current_loss, tf.compat.v1.trainable_variables(scope=None))\n    optimizer = tf.optimizers.Adam(learning_rate)\n    optimizer.apply_gradients(zip(grads,tf.compat.v1.trainable_variables(scope=None)))\n    \n\nWs, bs = [], []\nepochs = range(10)\nfor epoch in epochs:\n    Ws.append(model.W.numpy())\n    bs.append(model.b.numpy())\n    current_loss = loss(model(inputs), outputs)\n    \n    train(model, inputs, outputs, learning_rate = 0.1)\n    print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n        (epoch, Ws[-1], bs[-1], current_loss))\n\nplt.plot(epochs, Ws, 'r',\n         epochs, bs, 'b')\nplt.plot([TRUE_W] * len(epochs), 'r--',\n         [TRUE_B] * len(epochs), 'b--')\nplt.legend(['W', 'b', 'True W', 'True b'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nfrom tensorflow import keras\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import layers\n\n\nTRUE_W = 3.0\nTRUE_B = 2.0\nNUM_EXAMPLES = 1000\n\"\"\"\nclass MyModel(Model):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        \n    def __call__(self, x):\n        return self.W * x + self.b\n\"\"\"\ninputs_k  = keras.Input(shape = (NUM_EXAMPLES,))\noutputs_k = layers.Dense(1)(inputs_k)\nmodel = keras.Model(inputs=inputs_k, outputs=outputs_k)\n\ndef loss(predict_y, target_y):\n    return tf.reduce_mean(tf.square(predict_y - target_y))\n\n\ninputs = tf.random.normal(shape = [1,NUM_EXAMPLES])\nnoise   = tf.random.normal(shape=[1,NUM_EXAMPLES])\noutputs = inputs * TRUE_W + TRUE_B + noise\n\nprint('Current loss: %1.6f' % loss(model(inputs), outputs).numpy())\n#print(model.trainable_variables)\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n\ndef train(model, inputs, outputs, learning_rate):\n    with tf.GradientTape() as t:\n        current_loss = loss(model(inputs), outputs)\n    grads  = t.gradient(current_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\nepochs = range(10)\nfor epoch in epochs:\n    current_loss = loss(model(inputs), outputs)\n    train(model, inputs, outputs, learning_rate = 0.0001)\n    print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n        (epoch, Ws[-1], bs[-1], current_loss))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\ntf.keras.backend.clear_session()  # For easy reset of notebook state.\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ninputs = keras.Input(shape=(784,), name='digits')\nx = layers.Dense(64, activation='relu', name='dense_1')(inputs)\nx = layers.Dense(64, activation='relu', name='dense_2')(x)\noutputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n#print(model.trainable_variables)\n\na = tf.random.normal(shape = [1,784])\nprint(a.shape)\nprint(model.predict(a))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}